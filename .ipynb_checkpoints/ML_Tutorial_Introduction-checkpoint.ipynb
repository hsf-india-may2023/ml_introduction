{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211e2141",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Machine Learning for Particle Physicists\n",
    "\n",
    "\n",
    "## TIFR, May 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfad054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are Artificial Intelligence & Machine Learning?\n",
    "\n",
    "- Artificial Intelligence (AI): human-like, intelligent machines or programs\n",
    "- Machine Learning (ML): AI algorithms that learn from data instead of being explicitly human-programmed\n",
    "\n",
    "<div align=center>\n",
    "<br>\n",
    "    <img src=\"figs/AIML_Diagram.png\" width=\"350\"/>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94192c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of a ML Method: k-Nearest-Neighbor Classification \n",
    "\n",
    "<img align=\"right\" src=\"figs/KnnClassification.svg\" width=\"400\"/>\n",
    "\n",
    "1. Find k points nearest to a new, test point\n",
    "2. Classify the test point with majority vote\n",
    "\n",
    "- Decision / Parameters based on existing, labeled samples\n",
    "- No explicit programming of decision boundaries or parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bee8d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example of a ML Method: Support Vector Machine (linear)\n",
    "\n",
    "1. Decision boundary: $wx - b = 0$\n",
    "2. Optimize w and b to maximize separation in existing data\n",
    "\n",
    "<div align=center>\n",
    "<br>\n",
    "    <img src=\"figs/SVM_margin.jpg\" width=\"350\"/>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ee3ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree\n",
    "<img align=\"right\" src=\"figs/DecisionTree.png\" width=\"450\"/>\n",
    "\n",
    "- Extend cut-based selection\n",
    "    - Many events do not have all  \n",
    "    characteristics of signal/background\n",
    "    - Try not to rule out events  \n",
    "    failing a particular criterion\n",
    "- Keep events rejected by one  \n",
    "criterion and see whether other  \n",
    "criteria could help classify them properly\n",
    "- __Binary trees__ can be built with  \n",
    "branches splitting into many sub-branches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689335d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree Example\n",
    "\n",
    "<img align=\"right\" src=\"figs/Decision_Tree_-_survival_of_passengers_on_the_Titanic.jpg\" width=\"550\"/>\n",
    "\n",
    "- Tree of survival of Titanic passengers  \n",
    "\"sibsp\" = number of spouses/siblings\n",
    "- Probability of survival & percentage  \n",
    "of observations in the leaf\n",
    "- Summary: chances of survival good if \n",
    "    - a female \n",
    "    - a male <9.5 years & <3 siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4478b14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Boosting (the B of BDT)\n",
    "\n",
    "<img align=\"right\" src=\"figs/BDT_Boosting.png\" width=\"600\"/>\n",
    "\n",
    "- Adaptive Boosting __AdaBoost__\n",
    "    - combines multiple weak  \n",
    "    learners (single split)  \n",
    "    into a single strong learner  \n",
    "- Gradient Boosting __GradBoost__  \n",
    "    - Minimise overall loss with  \n",
    "    each additional tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50237698",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters of a BDT\n",
    "<img align=\"right\" src=\"figs/BDT_AlgoExample.png\" width=\"500\"/>\n",
    "\n",
    "- Number of estimators / trees T\n",
    "- Max Depth\n",
    "- Learning rate\n",
    "- Min events per leaf\n",
    "- Bagging\n",
    "- Random subset of features per cut\n",
    "- Pruning\n",
    "- Treatment of categorical variables\n",
    "###### Popular algorithms are XGBoost, LightGBM, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acaf9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are Neural Networks?\n",
    "\n",
    "<img align=\"right\" src=\"figs/Neural_network_example.svg\" width=\"275\"/>\n",
    "\n",
    "- A Neural Network (NN) is function whose computational  \n",
    "graph mimics the structure of biological neural systems\n",
    "- It is defined by:\n",
    "    - Architecture (computational graph)\n",
    "    - Parameters (weights in connections)\n",
    "    ###### Each circle is called a node or a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5968a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does a NN actually look like?\n",
    "A NN is a function, so it has an input and an output\n",
    "1. Linear transformation (including bias)  \n",
    "Can be summarized by a matrix of weights, $W$\n",
    "2. Activation function\n",
    "(some non-linear function applied at each node) \n",
    "Common examples include:  \n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}}$  \n",
    "$\\text{ReLU}(z) = \\text{max}(0,z)$  \n",
    "$\\text{Softplus}(z)=\\text{ln}(1+e^z)$  \n",
    "(activation function here is applied element-wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d8f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inside a Neural Network\n",
    "\n",
    "<div align=center>\n",
    "<br>\n",
    "    <img src=\"figs/NN_Diagram.jpg\" width=\"600\"/>\n",
    "<br>\n",
    "</div>\n",
    "\n",
    "This step is repeated multiple times  \n",
    "x(# of Nodes) x(# of Layers) x more...   \n",
    "Depends on the architecture of the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8259c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Universal Approximation Theorem (UAT)\n",
    "\n",
    "A NN with sufficiently many nodes can approximate any function with an arbitrarily small error!\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/UAT.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "   ##### Early proofs:  G. Cybenko, “Approximation by Superpositions of a Sigmoidal Function” (1989), K. Hornik, et al, “Multilayer Feedforward Networks are Universal Approximators” (1989), K. Horink, “Approximation Capabilities of Multilayer Feedforward Networks” (1991) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f455da4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the Universal Approximation Theorem\n",
    "Example: single-layer, wide sigmoidal NN\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/NN_UATFig.png\" width=\"800\"/>\n",
    "</div>    \n",
    "\n",
    "    - Linear transform + sigmoid: can approximate ~1 local point\n",
    "    - Many sigmoids combined ⇒ can approximate many points\n",
    "    - Generalizable to higher dimensions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a551bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing How NN’s Work: A Simple Example\n",
    "\n",
    "<img align=\"right\" src=\"figs/Neural_network_example_2.png\" width=\"325\"/>\n",
    "\n",
    "- Conventionally, <span style=\"color:orange\">__feature extraction__</span> is done  \n",
    "by humans with domain-/problem-specific expertise\n",
    "    - Experience, *a priori* physics knowledge, etc.\n",
    "    - Expensive, difficult\n",
    "- <span style=\"color:green\">__Linear models__</span> are easy to solve, but limited\n",
    "    - Closed form solution or convex optimization\n",
    "    - But linear, obviously...\n",
    "- __NNs can perform both and learn based on data__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a23e1bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing How Neutral Networks Work\n",
    "- Simple NN training live on browser: [Link](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.003&regularizationRate=0&noise=0&networkShape=3&seed=0.13116&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/NN_Visualization.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "- Visualizing hidden layers & decision boundaries: [Link](http://srome.github.io/Visualizing-the-Learning-of-a-Neural-Network-Geometrically/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de085a26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approaches to Learning and Supervision\n",
    "\n",
    "<img align=\"right\" src=\"figs/Supervision.png\" width=\"400\"/>\n",
    "\n",
    "- __Supervised learning__  \n",
    "    - Training data-set  \n",
    "    $ T = \\{(x_i, y_i) | i=1, ..., m\\} $\n",
    "    - Known inputs & corresponding outputs  \n",
    "    - MC simulation with known truths, etc.  \n",
    "- __Unsupervised learning__  \n",
    "    - Training data-set  \n",
    "    $ T = \\{x_i| i=1, ..., m\\} $  \n",
    "    - Just inputs, true outputs unknown  \n",
    "    - LHC data, anomaly detection, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24832bc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approaches to Learning and Supervision\n",
    "- __Supervised learning__  \n",
    "- __Unsupervised learning__  \n",
    "- __Semi- or weakly supervised learning__  \n",
    "    - Somewhere in between supervised & unsupervised  \n",
    "    - Partial information like mean output values or outputs known for subset of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d3e28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Neural Networks\n",
    "\n",
    "- To train NNs in supervised learning, we need: \n",
    "    - Training data-set: $ T = \\{(x_i, y_i) | i=1, ..., m\\} $\n",
    "    - Known inputs & corresponding outputs \n",
    "- Loss function: $ L(y',y) $\n",
    "    - How far a prediction $ y’ = NN(x) $ is from true $y$ \n",
    "\n",
    "With these, learning is now an __optimization problem__, given $T$, find the NN parameters that minimize the mean loss:\n",
    "\n",
    "$$L_{\\text{overall}} = \\frac{1}{m} \\sum^m_{i=1}L(NN(x_i),y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf1733",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to train a NN: Backpropagation\n",
    "\n",
    "- Optimize using gradient-descent type methods\n",
    "- $ L(y',y) $ is differentiable & NN is differentiable\n",
    "\n",
    "<img align=\"right\" src=\"figs/Backpropagation.png\" width=\"550\"/>\n",
    "\n",
    "$ \\rightarrow $ Gradients can be propagated backwards recursively with Chain Rule of Derivatives\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_j} = \\frac{\\partial L}{\\partial h_{j+1}}\\frac{\\partial h_{j+1}}{\\partial h_j} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9630df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Training NNs: Gradient Descent (GD) Algorithm\n",
    "1. Initialize NN with random parameters  \n",
    "$\\rightarrow$ typically standard normal dist.\n",
    "2. Make prediction $y_i’ = NN(x_i)$ over training data $T$  \n",
    "$\\rightarrow$ forward pass through NN\n",
    "3. Compute the loss and backpropagate the gradient  \n",
    "$\\rightarrow$ backward pass through NN\n",
    "4. Update parameters by: gradient $\\times$ learning rate\n",
    "5. Repeat many times...\n",
    "\n",
    "      Steps 1-4 are called an epoch, \\# of epochs = how long a NN is trained for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0293977",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice for Efficient & Successful Training\n",
    "<img align=\"right\" src=\"figs/GlobProb.png\" width=\"550\"/>\n",
    "\n",
    "One of the main difficulties in training NNs is that there can be __many local minima and saddle points in high dimensions__\n",
    "- Solutions / Work-arounds\n",
    "    - Stochastic gradient descent\n",
    "    - More advanced optimizers\n",
    "    - Regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4da39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent & Mini-Batch\n",
    "<img align=\"right\" src=\"figs/BGD_Diagram.png\" width=\"450\"/>\n",
    "\n",
    "Stochastic Gradient Descent (SGD)  \n",
    "    - Instead of one update per full $T$ (one batch), update per every sample  \n",
    "    - High variance, but a lot more steps  \n",
    "    - $|T|$ updates per epoch  \n",
    "\n",
    "<img align=\"right\" src=\"figs/GD_Diagram.png\" width=\"450\"/>\n",
    "\n",
    "Mini-batch gradient descent  \n",
    "    - Update per mini-batch of size $B$  \n",
    "    - Compromise between full batch GD & SGD  \n",
    "    - $|T| / B$ updates per epoch  \n",
    "\n",
    "SGD = Mini-batch GD with $B = 1$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ca716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Optimizers: Sparse / Vanishing gradients in certain parameters\n",
    "- Mitigate scale difference between different parameters  \n",
    "- Adaptive step sizes for different params. by keeping track of gradient sizes  \n",
    "    ###### Update equation for __AdaGrad__ method\n",
    "\\begin{gather}\n",
    "\\small{\n",
    "\\begin{bmatrix}\n",
    "\\theta^{(1)}_{t+1} \\\\\n",
    "\\theta^{(2)}_{t+1} \\\\\n",
    "\\vdots\\\\\n",
    "\\theta^{(m)}_{t+1} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\theta^{(1)}_{t} \\\\\n",
    "\\theta^{(2)}_{t} \\\\\n",
    "\\vdots\\\\\n",
    "\\theta^{(m)}_{t} \\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\eta}{\\sqrt{\\epsilon+G^{(1,1)}_t}}g^{(1)}_{t} \\\\\n",
    "\\frac{\\eta}{\\sqrt{\\epsilon+G^{(2,2)}_t}}g^{(2)}_{t} \\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\eta}{\\sqrt{\\epsilon+G^{(m,m)}_t}}g^{(m)}_{t} \\\\\n",
    "\\end{bmatrix}}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d837c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Advanced Optimizers: Momentum in certain directions\n",
    "- Keep info. about previous updates\n",
    "- This acts like “momentum”  \n",
    "    ###### Update equation for SGD with momentum\n",
    "\n",
    "<img align=\"right\" src=\"figs/movie10.gif\" width=\"500\"/>\n",
    "\n",
    "$V_t = \\beta V_{t-1} + \\alpha \\nabla_W L(W-\\beta V_{t-1}, X, y)$  \n",
    "$W = W-V_t$\n",
    "\n",
    "\n",
    "- Other popular modern optimizers RMSProp, Adam, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d193247",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Validation of NN Training\n",
    "\n",
    "- When training a NN, we want to quantify its performance $\\rightarrow$ measures are mean loss, accuracy, etc. smaller = better\n",
    "    - Use of __training data-set__ to quantify performance is prone to bias\n",
    "- __Validation data-set__ $\\rightarrow$ subset of data\n",
    "    - Not used for calculating gradients, used for monitoring purposes\n",
    "- __Test data-set__ $\\rightarrow$ subset of data\n",
    "    - Blinded during training, only for final performance measure\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/ValidationDatasets.png\" width=\"350\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d7bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Training Progress & Overfitting\n",
    "- Plot loss, accuracy, etc. over epochs, __both training & validation losses__  \n",
    "- __Overfitting to training set__ decreasing training loss, but not validation loss, learning training-set-specific biases\n",
    "    - Solution: __stop training, different NN model, different optimizer, etc__\n",
    "    \n",
    "<table><tr>\n",
    "<td> <img src=\"figs/Overfitting_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "<td> <img src=\"figs/Overfitting_3.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7807f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Training Progress & Overfitting\n",
    "- Plot loss, accuracy, etc. over epochs, __both training & validation losses__  \n",
    "- __Overfitting to training set__ decreasing training loss, but not validation loss, learning training-set-specific biases\n",
    "    - <font color='blue'>Concept is simple, but noticing it in practice is more subtle</font>\n",
    "<table><tr>\n",
    "<td> <img src=\"figs/Overfitting_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"figs/Overfitting_3.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309202a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### Performance Measures for Classification Problems\n",
    " \n",
    "Consider binary classification example\n",
    "\n",
    "|       | Actually Positive (P) | Actually Negative (N)|\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Predicted Positive      | TP      | FP |\n",
    "| Predicted Negative   | FN        | TN |\n",
    "\n",
    "- __Accuracy, ACC = (TP + TN) / (N + P)__  \n",
    "- __Signal efficiency $\\epsilon_\\text{S} = $ TP / P__ also called true positive rate (TPR), sensitivity  \n",
    "- __Background efficiency $\\epsilon_\\text{B} = $ FP / N__ also called false positive rate (FPR) \n",
    "- In High Energy Physics (HEP), often use __background rejection__ instead bkg. rej. $= 1 / $bkg. eff. \n",
    "    - Bkg. rej. of 100 = 1 in 100 background events pass as fake positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804aaaf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### ROC Curve (Receiver Operating Characteristic)\n",
    "- Plot of true positive rate/signal efficiency against false positive rate/background efficiency\n",
    "- In HEP, often use $1/\\epsilon_B$ against $\\epsilon_S \\rightarrow$ for 70% signal efficiency, 1000 bkg. rej \n",
    "    \n",
    "<table><tr>\n",
    "<td> <img src=\"figs/PerfMeasures_2.png\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"figs/PerfMeasures_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e97a47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flavors of NN Architectures: Convolutional Neural Network (CNN)\n",
    "NN input not limited to 1-dim. vector, great for images & pixelated geometries\n",
    "- Striding filters across axes\n",
    "    - Sliding window, convolutional integral\n",
    "    - Each filter = weight matrix + activation\n",
    "- Understands local geometry, translational invariance\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/CNN.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c60f0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CNN Example: Jets in ATLAS\n",
    "- Calorimeter is inherently a pixelated detector\n",
    "- Value in each pixel = energy deposit (ECAL + HCAL)\n",
    "- Good for finding clusters\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/JetImages.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb207b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Network (RNN)\n",
    "\n",
    "- NN can handle variable length sequences\n",
    "- Previous outputs are re-entered as inputs\n",
    "- Understands order and context (has memory)\n",
    "- Great for natural languages, time-series prediction\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/RNN.png\" width=\"700\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7eb6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN Example: Jet Flavor in ATLAS\n",
    "- Jet = series of tracks, topoclusters, PFlow, etc.\n",
    "- Each input = particle four-vector\n",
    "- Needs some ordering (e.g. leading-$p_T$)\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/figs_RNNIP_1.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb46c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Sets - Enforcing permutation invariance\n",
    "- RNN is requires an order, use sets instead of sequences (no preferred order) i.e. permutation-invariant\n",
    "- __Deep Sets__\n",
    "<div align=center>\n",
    "    <img src=\"figs/DeepSets.png\" width=\"600\"/>\n",
    "</div>\n",
    "<div align=center>\n",
    "    <img src=\"figs/DeepSets_Diagram.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846c2c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Sets Example: Jet Flavor in ATLAS\n",
    "- Jet = series of tracks, topoclusters, PFlow, etc.\n",
    "- Each input = particle four-vector\n",
    "- No order needed\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/DeXTer_DeepSets.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f7e39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative Adversarial Network (GAN)\n",
    "\n",
    "- Allows to combine multiple NNs for more complicated tasks\n",
    "- Two NNs: generator G and discriminator D\n",
    "- __Compete when training: one fakes, one distinguishes__\n",
    "- Eventually, G becomes good at __generating realistic data__\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/GAN.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e54ff6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN Example: Jet Flavor in ATLAS\n",
    "- Remove biases that make calibration difficult and limit applicability \n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/DeXTer_AdversarialNN.png\" width=\"800\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609429c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN Example: Jet Simulation\n",
    "- Generate realistic jets in calorimeters\n",
    "- Computationally cheap, fast alternative to full MC\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/CaloGAN.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37aa465",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rise of AI / ML / DL: Just a Regression?\n",
    "\n",
    "- Sure, a NN is a universal approximator...\n",
    "- But there are many more!\n",
    "    - Taylor series (for smooth functions)\n",
    "    - Fourier series , Bernstein polynomial (over bounded regions)\n",
    "- __So, is the NN just another way of fitting a function? ⇒ Yes, but...__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780d42e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When is / What makes NN better?\n",
    "- __With high-dim. input data__\n",
    "    - Polynomial function would scale like $n^k$\n",
    "    - NNs known to scale more efficiently, in some cases polynomially (shown by empirical successes)\n",
    "    - NN structure & backprop. ⇒ easier to optimize \n",
    "- __Easy to train and use__\n",
    "    - Surprisingly simple nowadays\n",
    "    - No / little human engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb66cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Hardware: Graphics Processing Unit (GPU)\n",
    "- Single instruction, multiple data (SIMD) architecture\n",
    "- Specialized for parallel, repetitive tasks like matrix multiplication\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/GPU_CPU.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89183e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Hardware: Tensor Processing Unit (TPU)\n",
    "- Google proprietary technology \n",
    "- Application specific ASIC designed for high volume of low precision computation \n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"figs/Tensor_Processing_Unit_3.0.jpg\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "- Specialized for NN computation — matrix mult, convolution, and activation\n",
    "\n",
    "__Other NN-specific hardware options are rising in popularity__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983259bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Hardware: Computational Resources\n",
    "- Faster Internet—connection is less of a bottleneck\n",
    "- Cloud & sever availability made high-performance computing more available for all\n",
    "- Amazon, Microsoft, Google, national computational facilities, etc.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"figs/TIFR_Computing.png\" alt=\"Drawing\" style=\"width: 140px;\"/> </td>\n",
    "<td> <img src=\"figs/LHCOne_India.png\" alt=\"Drawing\" style=\"width: 700px;\"/> </td>\n",
    "</tr></table>\n",
    "<div align=center>\n",
    "    <img src=\"figs/LHCOne_India.png\" width=\"700\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0c1d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outlook for Physicists\n",
    "- AI/ML very active area with many applications to Particle Physics\n",
    "- Lot of knowledge/resources still to __absorb__ from broader AI/ML community\n",
    "    - __Active communication & collaboration__ between academia & industry\n",
    "- Continue ML application R&D in data analysis\n",
    "    - Replace existing heuristic-based tools\n",
    "    - Update with newer methods\n",
    "    - Train/educate more physicists with ML-fluency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499454a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newer / less familiar research directions - Many areas!\n",
    "- ML-specific hardware (e.g. low-level trigger with FPGAs)\n",
    "<div align=center>\n",
    "    <img src=\"figs/HeterogeneousArchitectures.png\" width=\"300\"/>\n",
    "</div>\n",
    "- Simulation (GAN’s, differentiable simulators, diffusion models)\n",
    "- Design and operation of experiments\n",
    "- Resource management (in large servers and data storage centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e67c41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifiers at the Large Hadron Collider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532278c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Acknowledgements, Links to Materials / Images Borrowed\n",
    "\n",
    "Includes content of slides, figures, materials from Sanha Cheong, Aishik Ghosh, Michael Kagan, Yann Coadou, Yuan-Tang Chou, Brij Kishor, David Rousseau, James Catmore\n",
    "\n",
    "https://www.researchgate.net/figure/Figure-Artificial-Intelligence-Machine-Learning-and-Deep-Learning_fig3_340684782\n",
    "\n",
    "https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg \n",
    "\n",
    "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#/media/File:KnnClassification.svg \n",
    "\n",
    "https://en.wikipedia.org/wiki/Support-vector_machine#/media/File:SVM_margin.png \n",
    "\n",
    "https://towardsdatascience.com/introduction-to-math-behind-neural-networks-e8b60dbbdeba \n",
    "\n",
    "https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586 \n",
    "\n",
    "https://towardsdatascience.com/back-propagation-simplified-218430e21ad0 \n",
    "\n",
    "https://www.researchgate.net/figure/Gradient-Descent-Stuck-at-Local-Minima-18_fig4_338621083 \n",
    "\n",
    "https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0 \n",
    "\n",
    "https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461\n",
    "\n",
    "https://github.com/Jaewan-Yun/optimizer-visualization\n",
    "\n",
    "https://www.linkedin.com/pulse/supervised-vs-unsupervised-learning-whats-difference-smriti-saini/ \n",
    "\n",
    "https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/ \n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "https://indico.tifr.res.in/indico/getFile.py/access?contribId=41&sessionId=15&resId=0&materialId=slides&confId=8398\n",
    "\n",
    "https://arxiv.org/pdf/1712.10321.pdf\n",
    "\n",
    "https://cds.cern.ch/record/2825434/files/ATL-PHYS-PUB-2022-042.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1511.05190.pdf\n",
    "\n",
    "https://indico.in2p3.fr/event/26179/contributions/106549/attachments/70511/100013/SoS_DT220517.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453b75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
